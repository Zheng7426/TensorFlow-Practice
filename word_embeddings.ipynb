{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 2020\n",
    "train_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir, \n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training', \n",
    "    seed=seed)\n",
    "val_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1: positive, 0: negative</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b'Although this movie has some weaknesses, it is worth seeing. I chose it because of the cast, and applaud Bonham Carter and Branagh for choosing roles different from those they have taken in the past. Both portray very troubled people, complete with warts, but make them likeable because of their humanity. The story is touching, but it is the performances that soar. Bonham Carter\\'s \"Jane\" is a remarkable achievement, whose quest for romance opened my eyes to aspects of being disabled that I had not thought of before, but was interesting as well for other reasons. I felt the movie ended too abruptly, but better that than a drawn out emotionally manipulative ending (see \"Stepmom.\") The very real English setting added to my enjoyment - it was England in the 90\\'s, both urban and rural, without being depressing.'\n",
      "1 b\"This film pulls you in from the get-go because it grabs our attention by acknowledging, yeah, that this story is opening with a clich\\xc3\\xa9 \\xc2\\x96 a funeral.<br /><br />In hands other than Judi's I wouldn't have given it an 8 as this material has been done over and over again: The great reunion of a once famous, pick one please, team, army platoon, theatre group, singers, band.<br /><br />But this movie never stoops to cheap sentimentalization, and when you think it is going to it swoops off in another direction. A case in point is the flowers that are sent by an admirer to Judi.<br /><br />The band members are an interesting group and ride above the clich\\xc3\\xa9s too. One is in jail, one has found religion, one is an alkie, and one has sunk into dementia. But the joie de vivre rediscovered by Judi, ignited by her granddaughter's interest, carries us along and makes us overlook the sometimes simplistic nature of the plot.<br /><br />The cast are a who's who of talent, Leslie Caron, the incomparable jazzist Cleo Laine with her amazing high notes, a last performance from Joan Sims, brava Joan, a cute as a button flirtatious Ian Holm having a ball, and Olympia Dukakis as a money-grabbing divorc\\xc3\\xa9e living in the highlands of Scotland with her ghillie and her whiskey, The closing scene is standard Hollywoodland fare, the judgmental children of the star converted to fun-loving supporters, the old lovers reunited, the youngsters swept up in the old timers' music. Life should be this simple. But I would watch it again, and intend to, with my own granddaughter. For in the right hands, sometimes one just loves these brazen old clich\\xc3\\xa9s. 8 out of 10.\"\n",
      "0 b\"Uma Thurman plays Sissy, a young woman with a gypsy spirit (and freakishly large thumbs) who hitchhikes cross-country, eventually finding her true place amongst a group of peyote-enlightened cowgirls on a ranch devoted to preserving the Whooping Crane; Rain(bow) Phoenix is their lesbian leader, Bonanza Jellybean, who falls in love with Sissy, thumbs or not. Gus Van Sant directed and adapted Tom Robbins' book, but his satire has no primary target and just skitters all over the map, like Sissy (maybe that was his goal, but it's not involving for an audience). Notorious box-office flop wasn't so much panned as it was ignored, and one can see why: it's a series of sketches in search of a plot, and the performances, directorial touches and cinematography are all variable. Thurman is a stitch posing alongside the highway trying to get a ride, but this pretty much put the kibosh on Phoenix's career. Writer Buck Henry (who didn't write this, but perhaps should have) gives the most assured performance as the doctor who works on one of those thumbs.<br /><br />Two thumbs down.\"\n",
      "1 b'A delightful if somewhat predictable TV movie, though I admit a little bias -- as far as I\\'m concered, the more Gene Wilder in this world the better. I\\'d love to see numerous additional movies detailing the adventures of Larry \"Cash\" Carter!'\n",
      "1 b\"Always fancied this film from the video cover. Eventually got round to buying it for a fiver in a sale and boy what a film. A simply stunning performance from all of the case and it's filmed so beautifully. Even at times from a distance so you can barely hear what the dialogue is, as if you really are that distance away picking up bits of the tale. It's really moving, frequently amusing and very watchable. Not much dialogue but is filmed in such a way that you feel so much throuout. A 9/10 from me. A must see.\"\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_data.take(1):\n",
    "    for i in range(5):\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Using the Embedding Layer</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 word vocabulary into 5 dimensions\n",
    "# Random initialization, similarities would be encoded by the word embeddings.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)\n",
    "\n",
    "result = embedding_layer(tf.constant([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04029671, -0.02687234,  0.01883718, -0.00956687, -0.03077844],\n",
       "       [ 0.04513909,  0.00062316,  0.00826148, -0.01962583, -0.01752422],\n",
       "       [ 0.00568271,  0.0040072 , -0.02780952,  0.04252643,  0.01310693]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.00080323 -0.01714882  0.0058575  -0.0066018  -0.03084812]\n",
      "  [ 0.04029671 -0.02687234  0.01883718 -0.00956687 -0.03077844]\n",
      "  [ 0.04513909  0.00062316  0.00826148 -0.01962583 -0.01752422]]\n",
      "\n",
      " [[ 0.00568271  0.0040072  -0.02780952  0.04252643  0.01310693]\n",
      "  [ 0.00251956 -0.03746142  0.02632819  0.02475611 -0.01004137]\n",
      "  [ 0.03897863 -0.03387398 -0.04908519 -0.01402403  0.03230834]]]\n",
      "(2, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1,2,3]))\n",
    "result.numpy()\n",
    "\n",
    "result_2 = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
    "print(result_2.numpy())\n",
    "print(result_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Text-only dataset, labels removed.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Definition</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim, name='embedding'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 1/20 [>.............................] - ETA: 0s - loss: 0.6932 - accuracy: 0.4951WARNING:tensorflow:From C:\\Users\\huangz55\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "20/20 [==============================] - 11s 548ms/step - loss: 0.6922 - accuracy: 0.4995 - val_loss: 0.6906 - val_accuracy: 0.5020\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 10s 522ms/step - loss: 0.6882 - accuracy: 0.4995 - val_loss: 0.6854 - val_accuracy: 0.5020\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 10s 523ms/step - loss: 0.6808 - accuracy: 0.4995 - val_loss: 0.6766 - val_accuracy: 0.5020\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 10s 497ms/step - loss: 0.6691 - accuracy: 0.4995 - val_loss: 0.6632 - val_accuracy: 0.5020\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 10s 495ms/step - loss: 0.6520 - accuracy: 0.4995 - val_loss: 0.6444 - val_accuracy: 0.5020\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 10s 500ms/step - loss: 0.6287 - accuracy: 0.5069 - val_loss: 0.6205 - val_accuracy: 0.5342\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 10s 497ms/step - loss: 0.6004 - accuracy: 0.5824 - val_loss: 0.5930 - val_accuracy: 0.6010\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 10s 502ms/step - loss: 0.5683 - accuracy: 0.6505 - val_loss: 0.5629 - val_accuracy: 0.6686\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 10s 500ms/step - loss: 0.5339 - accuracy: 0.7082 - val_loss: 0.5327 - val_accuracy: 0.7206\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 10s 498ms/step - loss: 0.4997 - accuracy: 0.7588 - val_loss: 0.5048 - val_accuracy: 0.7330\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 10s 506ms/step - loss: 0.4680 - accuracy: 0.7792 - val_loss: 0.4796 - val_accuracy: 0.7576\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 10s 495ms/step - loss: 0.4392 - accuracy: 0.7965 - val_loss: 0.4585 - val_accuracy: 0.7778\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 10s 494ms/step - loss: 0.4136 - accuracy: 0.8167 - val_loss: 0.4411 - val_accuracy: 0.7838\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 10s 493ms/step - loss: 0.3914 - accuracy: 0.8289 - val_loss: 0.4267 - val_accuracy: 0.7950\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 10s 490ms/step - loss: 0.3715 - accuracy: 0.8386 - val_loss: 0.4149 - val_accuracy: 0.8008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2075357c0d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    train_data, \n",
    "    validation_data=val_data,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 40300), started 0:27:25 ago. (Use '!kill 40300' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a74cc4db4108f058\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a74cc4db4108f058\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learned Embeddings of shape(vocab_size, embedding_dimension)\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0: continue\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
